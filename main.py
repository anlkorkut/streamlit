"""
This Streamlit app interfaces with OpenAI's API to answer questions from a given PDF.
It extracts text from a specified PDF and uses OpenAI's completions to generate answers.
It includes user inputs and settings in the sidebar, with a structured display in the main panel.
OpenAI's API to provide answers to questions based on the content of a PDF file. It utilizes
a text splitter to handle large PDF content and an OpenAI API wrapper to manage API calls.

Functions:
  extract_text_from_pdf(pdf_path: str) -> str
  get_answer_openai(question: str, context: str, chunk_size: int) -> str

"""
import streamlit as st
import PyPDF2
from langchain.text_splitter import RecursiveCharacterTextSplitter
from src.openai_utils.openai_wrapper import OpenAIWrapper, Message
from src.prompts import rag_demo_prompts
from src.vectordb.vectordb_wrapper import VectorDBWrapper

_OPENAI_WRAPPER = OpenAIWrapper()
_VECTOR_DB = VectorDBWrapper()


def extract_text_from_pdf(pdf_path: str) -> str:
  """
  Extracts and concatenates text from all pages of a specified PDF file.

  This function opens a PDF file, reads each page, and extracts the text.
  It then concatenates the text from all pages into a single string, separated by spaces.

  Args:
      pdf_path (str): The file path of the PDF from which to extract text.

  Returns:
      str: A single string containing the concatenated text of all pages in the PDF.
  """
  with open(pdf_path, "rb") as file:
    reader = PyPDF2.PdfReader(file)
    return " ".join([page.extract_text() for page in reader.pages])

_PDF_PATH = "apps/rag_demo/data/demo.pdf"
PDF_CONTENT = extract_text_from_pdf(_PDF_PATH)

def get_answer_openai(question: str, context: str, chunk_size: int = 1024) -> str:
  """
  Generates an answer to a specified question based on a given context by querying the OpenAI API.

  This function splits the provided context into smaller chunks and sends each chunk, along with the
  question, to the OpenAI API. It processes the chunks sequentially and returns the response for the
  first chunk that yields a confident answer from the API.

  Args:
      question (str): The question for which an answer is sought. It should be a clear question.
      context (str): The text context used to answer the question. Extracted from a PDF document.
      chunk_size (int, optional): Size of each chunk of context text characters. Defaults to 1024.
                                  Chekcs how the context is split up for the API, as there are
                                  limits to how much text can be processed in a single API request.

  Returns:
      str: The answer generated by the OpenAI API, based on question and context. If the API fails
            to give a confident answer from chunks, a default message is returned indicating that no
            relevant answer could be found.
  """
  text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=50, length_function=len)
  context_chunks = text_splitter.split_text(context)

  for chunk in context_chunks:
    messages = [
      Message(role="system", content=rag_demo_prompts.system_prompt()),
      Message(role="user", content=rag_demo_prompts.content_prompt(chunk, question))
    ]

    print("Sending the following messages to OpenAI: ")
    for message in messages:
      print(f"Role: {message.role}, Content: {message.content}")

    response = _OPENAI_WRAPPER.conversational_completion(
      messages=messages,
      model="gpt-4",
      temperature=0.1,
    )

    return rag_demo_prompts.parse_response(response)

  return "I'm sorry, I wasn't able to find a relevant answer to your question in the text."

def main():

  st.title("OpenAI-based Question Answering from PDF")
  user_input = st.text_input("ðŸš€ Hi human! I am your smart AI. How can I help you today?")

  st.sidebar.header("Settings")
  model_temperature = st.sidebar.slider("Model Temperature", 0.0, 1.0, 0.5)
  max_tokens = st.sidebar.slider("Max Tokens", 50, 300, 150)

  if user_input:
    with st.spinner("Generating answer..."):
      answer = get_answer_openai(user_input, PDF_CONTENT)
      st.write(answer)

  def display_footer():
    st.markdown("---")
    st.markdown(
      """
      Made with ðŸ’™ by [Anil Korkut](https://neuralbridge.ai) |
      [Privacy Policy](https://neuralbridge.ai) |
      [Terms of Service](https://neuralbridge.ai) |
      [Contact Us](mailto:anil@neuralbridge.ai)
      """,
      unsafe_allow_html=True
    )
  display_footer()

if __name__ == "__main__":
  main()
